# Image-Captioning
<ul>
  <li> Implemented Encoder-Decoder architecture for image captioning on MS COCO dataset</li>
  <li> Used SOTA Deep CNN models as Image Encoder and Decoder only Transformer architecture with Masked Attention and Cross Attention as Decoder</li>
  <li> Compared Resnet101 and Vision Transformer as feature extractor for image captioning</li>
  <li> Performed Dynamic Quantization i.e Post Training Quantization to reduce size and latency of trained model on resource deficit devices</li>
</ul>
